\chapter{考察}
\label{ch:discussion}
本章では、本研究を実社会へ適用する際に想定される効果及び課題について記述する。

\section{多様化する媒体への対応}
\cref{ch:gen_com}では、記事に予想されるコメントを生成することで検出を補助するモデルを提案した。
本モデルの利点として、他の媒体への拡張が容易である点が挙げられる。
コメントはどの媒体の情報に対して共通して自然言語として使われるため、投稿とコメントがセットで提供されているデータセットがあれば、画像・動画・音声問わずどの形式の偽情報にも適用が可能となる。

\section{大規模言語モデルによる影響}
大規模言語モデル(LLM)の性質として、事実に基づかない情報を提供するハルシネーション(Hallucination)が指摘されている \cite{Alkaissi2023-bo}。
これはモデルの学習及び推論・改善において文章の自然さが優先され、事実に基づくかがあまり確認されていない点が理由として考えられる。

さらに、偽情報提供側がLLMを利用した意図的にミスリードを狙った投稿への対策も必要である。
\cref{ch:rel_res}の通り、既にThai Leら \cite{9338282}によってコメントを考慮した検出モデルを誤認させる手法が提案されている。
よって、学習において投稿されたコメントが実際に利用者によって書かれたか考慮する必要が生じている。

これらの問題点は知識ベースをLLMに組み込むことで改善を目指した研究も紹介されている \cite{10.1145/3512467}。
しかしながら、LLMの発展によって人間による文章かAIによる文章か見分けが難しくなっている点も指摘されている \cite{Elkhatat2023,chen2023can}点から、
自然言語処理における生成文章検出タスクのさらなる発展が求められている。

\section{動画への個別対応}
\cref{ch:introduction}の通り、本研究では偽情報動画の流布が他媒体と比べて多くないとして対象から除外した。
一方で、動画生成技術も発展を続けておりSora \cite{videoworldsimulators2024}に代表されるような高精細な映像も生成可能になりつつある。
開発したOpenAIは安全対策としてC2PAと呼ばれる電子透かし \cite{C2PA}の導入や、
生成プロンプトの内容によって偽情報生成を防止するシステム \cite{AI_2023}の適用を約束している \cite{AI_2024}。

しかしながら動画に限らず生成AI全体において、既存の防止システムの突破(jailbreak)を目的とした研究が幾つかある \cite{NEURIPS2023_fd661313,shayegani2024jailbreak}ため、
偽情報生成・検出と同様いたちごっこの様相を呈している。
また、今後ローカル環境での生成が可能になった場合は防止システムによる抑止が難しい点からも、動画生成側の技術発展による偽動画による偽情報投稿が今後急激に増える点が予想される。
検出においては、データセット作成において生成例が必要である点から実際の動画生成モデルの提供を待たなければならないが、今後個別対応が必要と考える。

\section{プラットフォームへの依存}
SNS上の検出を目指した場合、SNSプラットフォームへの依存は不可避の要素である。
しかしながら、プラットフォーム側の姿勢の変化により研究の障壁が大幅に変動する問題がある。
特に$\mathbb{X}$(旧Twitter)はイーロン・マスク氏による買収前は研究目的であれば投稿の取得が無償で行えたものの、
本論文執筆現在は月額課金として1万投稿/月に100米ドル、100万投稿/月に5000米ドル \cite{Twitter}と高額な料金が必要である。
このようなプラットフォーム側の姿勢によって、偽情報対策を目指す活動が抑制される可能性を危惧している。
